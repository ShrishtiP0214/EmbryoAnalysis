{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5c88f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageFile, ImageFilter\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure  # For SSIM metric\n",
    "import lpips  # For predefined perceptual loss\n",
    "from pytorch_msssim import SSIM  # For SSIM loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0adc248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Utility function to find common embryo IDs across directories\n",
    "def get_common_embryo_ids(base_paths):\n",
    "    sets_of_ids = []\n",
    "    for path in base_paths:\n",
    "        subfolders = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "        sets_of_ids.append(set(subfolders))\n",
    "    common_ids = set.intersection(*sets_of_ids)\n",
    "    return sorted(list(common_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7132be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2e4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        def CBR(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.enc1 = CBR(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2 = CBR(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc3 = CBR(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = CBR(256, 512)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.att3 = AttentionBlock(F_g=256, F_l=256, F_int=128)\n",
    "        self.dec3 = CBR(512, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.att2 = AttentionBlock(F_g=128, F_l=128, F_int=64)\n",
    "        self.dec2 = CBR(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.att1 = AttentionBlock(F_g=64, F_l=64, F_int=32)\n",
    "        self.dec1 = CBR(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "\n",
    "        b = self.bottleneck(self.pool3(e3))\n",
    "\n",
    "        d3 = self.up3(b)\n",
    "        e3 = self.att3(d3, e3)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        e2 = self.att2(d2, e2)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        e1 = self.att1(d1, e1)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        return self.final(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52e6fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbryoFocusStackDataset(Dataset):\n",
    "    def __init__(self, base_paths, f0_path, embryo_ids, is_train=False):\n",
    "        self.base_paths = base_paths\n",
    "        self.f0_path = f0_path\n",
    "        self.embryo_ids = embryo_ids\n",
    "        self.is_train = is_train\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embryo_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embryo_id = self.embryo_ids[idx]\n",
    "        focal_pil_images = []\n",
    "\n",
    "        for path in self.base_paths:\n",
    "            embryo_subfolder = os.path.join(path, embryo_id)\n",
    "            image_files = sorted([f for f in os.listdir(embryo_subfolder) \n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            if not image_files:\n",
    "                raise FileNotFoundError(f\"No image found in {embryo_subfolder}\")\n",
    "            img_path = os.path.join(embryo_subfolder, image_files[0])\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('L')\n",
    "                focal_pil_images.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                focal_pil_images.append(Image.new('L', (256, 256), 0))\n",
    "\n",
    "        f0_subfolder = os.path.join(self.f0_path, embryo_id)\n",
    "        f0_image_files = sorted([f for f in os.listdir(f0_subfolder) \n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        if not f0_image_files:\n",
    "            raise FileNotFoundError(f\"No F0 image found in {f0_subfolder}\")\n",
    "        f0_img_path = os.path.join(f0_subfolder, f0_image_files[0])\n",
    "        try:\n",
    "            f0_image = Image.open(f0_img_path).convert('L')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading F0 image {f0_img_path}: {e}\")\n",
    "            f0_image = Image.new('L', (256, 256), 0)\n",
    "\n",
    "        if self.is_train:\n",
    "            angle = random.uniform(-30, 30)\n",
    "            flip_h = random.random() < 0.5\n",
    "            flip_v = random.random() < 0.5\n",
    "        else:\n",
    "            angle = 0\n",
    "            flip_h = False\n",
    "            flip_v = False\n",
    "\n",
    "        transformed_images = []\n",
    "        for img in focal_pil_images:\n",
    "            img = TF.rotate(img, angle)\n",
    "            if flip_h:\n",
    "                img = TF.hflip(img)\n",
    "            if flip_v:\n",
    "                img = TF.vflip(img)\n",
    "            img = self.preprocess(img)\n",
    "            transformed_images.append(img)\n",
    "\n",
    "        f0_image = TF.rotate(f0_image, angle)\n",
    "        if flip_h:\n",
    "            f0_image = TF.hflip(f0_image)\n",
    "        if flip_v:\n",
    "            f0_image = TF.vflip(f0_image)\n",
    "        target_tensor = self.preprocess(f0_image)\n",
    "\n",
    "        focal_tensors = transformed_images\n",
    "        input_tensor = torch.cat(focal_tensors, dim=0)\n",
    "\n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d829f902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\Embryo\\embryo_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Projects\\Embryo\\embryo_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: c:\\Projects\\Embryo\\embryo_env\\Lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "l1_loss_fn = nn.L1Loss()  # Predefined L1 Loss\n",
    "perceptual_loss_fn = lpips.LPIPS(net='vgg')  # Predefined Perceptual Loss using LPIPS\n",
    "ssim_loss_fn = SSIM(data_range=1.0, size_average=True, channel=1)  # Predefined SSIM Loss\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0)  # SSIM metric for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "224a2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=100, device='cuda'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 20\n",
    "\n",
    "    # Move loss functions to device\n",
    "    perceptual_loss_fn.to(device)\n",
    "    ssim_loss_fn.to(device)\n",
    "    ssim_metric.to(device)\n",
    "\n",
    "    # Lists to store SSIM scores for plotting\n",
    "    train_ssim_scores = []\n",
    "    val_ssim_scores = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        running_train_ssim = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute losses\n",
    "            l1_loss = l1_loss_fn(outputs, targets)\n",
    "            perceptual_loss = perceptual_loss_fn(outputs, targets).mean()  # LPIPS returns a tensor\n",
    "            ssim_loss = 1 - ssim_loss_fn(outputs, targets)  # SSIM loss (1 - SSIM to convert to loss)\n",
    "            loss = 0.3 * l1_loss + 0.3 * perceptual_loss + 0.4 * ssim_loss  # Weighted combination\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Compute SSIM metric\n",
    "            ssim_value = ssim_metric(outputs, targets)\n",
    "            running_train_ssim += ssim_value.item() * inputs.size(0)\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_ssim = running_train_ssim / len(train_loader.dataset)\n",
    "        train_ssim_scores.append(train_ssim)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_ssim = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute losses\n",
    "                l1_loss = l1_loss_fn(outputs, targets)\n",
    "                perceptual_loss = perceptual_loss_fn(outputs, targets).mean()\n",
    "                ssim_loss = 1 - ssim_loss_fn(outputs, targets)\n",
    "                loss = 0.3 * l1_loss + 0.3 * perceptual_loss + 0.4 * ssim_loss\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Compute SSIM metric\n",
    "                ssim_value = ssim_metric(outputs, targets)\n",
    "                running_val_ssim += ssim_value.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_ssim = running_val_ssim / len(val_loader.dataset)\n",
    "        val_ssim_scores.append(val_ssim)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train SSIM: {train_ssim:.4f}, Val SSIM: {val_ssim:.4f}\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'embryo_unet_fusion.pth')\n",
    "            print(f\"  [*] Model saved at epoch {epoch+1}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= max_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    return train_ssim_scores, val_ssim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed85fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_embryo(model, image_paths, transform, device='cuda'):\n",
    "    model.eval()\n",
    "    focal_tensors = []\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).convert('L')\n",
    "        img_tensor = transform(img)\n",
    "        focal_tensors.append(img_tensor)\n",
    "    \n",
    "    input_tensor = torch.cat(focal_tensors, dim=0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    output_image = output.squeeze(0).cpu()\n",
    "    fused_pil = transforms.ToPILImage()(output_image)\n",
    "    \n",
    "    fused_pil = fused_pil.filter(ImageFilter.UnsharpMask(radius=2, percent=200, threshold=3))\n",
    "    fused_pil = TF.adjust_contrast(fused_pil, contrast_factor=1.5)\n",
    "    return fused_pil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2037a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    base_paths = [\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F15\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-15\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F30\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-30\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F45\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-45\"\n",
    "    ]\n",
    "    f0_path = r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset\"\n",
    "    \n",
    "    embryo_ids = get_common_embryo_ids(base_paths + [f0_path])\n",
    "    print(f\"Found {len(embryo_ids)} embryo IDs: {embryo_ids[:5]} ...\")\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    train_size = int(train_ratio * len(embryo_ids))\n",
    "    train_indices = random.sample(range(len(embryo_ids)), train_size)\n",
    "    val_indices = [i for i in range(len(embryo_ids)) if i not in train_indices]\n",
    "    embryo_ids_train = [embryo_ids[i] for i in train_indices]\n",
    "    embryo_ids_val = [embryo_ids[i] for i in val_indices]\n",
    "    \n",
    "    train_dataset = EmbryoFocusStackDataset(base_paths, f0_path, embryo_ids_train, is_train=True)\n",
    "    val_dataset = EmbryoFocusStackDataset(base_paths, f0_path, embryo_ids_val, is_train=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, num_workers=0, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, num_workers=0, shuffle=False)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = UNet(in_channels=6, out_channels=1).to(device)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    train_ssim_scores, val_ssim_scores = train_model(model, train_loader, val_loader, num_epochs=100, device=device)\n",
    "    print(\"Training complete. Best model saved as 'embryo_unet_fusion.pth'.\")\n",
    "\n",
    "    # Print SSIM scores for plotting\n",
    "    print(\"\\nSSIM Scores for Plotting:\")\n",
    "    print(\"Train SSIM Scores:\", train_ssim_scores)\n",
    "    print(\"Validation SSIM Scores:\", val_ssim_scores)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08b0f093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 704 embryo IDs: ['AA83-7', 'AAL839-6', 'AB028-6', 'AB91-1', 'AC264-1'] ...\n",
      "Using device: cuda\n",
      "Starting training...\n",
      "Epoch [1/100] Train Loss: 0.3514, Val Loss: 0.2197, Train SSIM: 0.4308, Val SSIM: 0.6166\n",
      "  [*] Model saved at epoch 1\n",
      "Epoch [2/100] Train Loss: 0.2058, Val Loss: 0.2210, Train SSIM: 0.6260, Val SSIM: 0.6283\n",
      "Epoch [3/100] Train Loss: 0.1709, Val Loss: 0.1619, Train SSIM: 0.6827, Val SSIM: 0.7007\n",
      "  [*] Model saved at epoch 3\n",
      "Epoch [4/100] Train Loss: 0.1596, Val Loss: 0.1567, Train SSIM: 0.6978, Val SSIM: 0.7004\n",
      "  [*] Model saved at epoch 4\n",
      "Epoch [5/100] Train Loss: 0.1527, Val Loss: 0.1059, Train SSIM: 0.7074, Val SSIM: 0.8305\n",
      "  [*] Model saved at epoch 5\n",
      "Epoch [6/100] Train Loss: 0.1159, Val Loss: 0.0953, Train SSIM: 0.8064, Val SSIM: 0.8520\n",
      "  [*] Model saved at epoch 6\n",
      "Epoch [7/100] Train Loss: 0.1258, Val Loss: 0.1086, Train SSIM: 0.7744, Val SSIM: 0.8124\n",
      "Epoch [8/100] Train Loss: 0.1171, Val Loss: 0.1291, Train SSIM: 0.7977, Val SSIM: 0.7579\n",
      "Epoch [9/100] Train Loss: 0.1029, Val Loss: 0.0876, Train SSIM: 0.8340, Val SSIM: 0.8696\n",
      "  [*] Model saved at epoch 9\n",
      "Epoch [10/100] Train Loss: 0.0958, Val Loss: 0.0888, Train SSIM: 0.8524, Val SSIM: 0.8685\n",
      "Epoch [11/100] Train Loss: 0.0915, Val Loss: 0.0878, Train SSIM: 0.8629, Val SSIM: 0.8669\n",
      "Epoch [12/100] Train Loss: 0.0912, Val Loss: 0.1088, Train SSIM: 0.8685, Val SSIM: 0.8395\n",
      "Epoch [13/100] Train Loss: 0.0994, Val Loss: 0.0944, Train SSIM: 0.8443, Val SSIM: 0.8621\n",
      "Epoch [14/100] Train Loss: 0.0888, Val Loss: 0.1004, Train SSIM: 0.8722, Val SSIM: 0.8450\n",
      "Epoch [15/100] Train Loss: 0.0883, Val Loss: 0.0860, Train SSIM: 0.8707, Val SSIM: 0.8707\n",
      "  [*] Model saved at epoch 15\n",
      "Epoch [16/100] Train Loss: 0.0863, Val Loss: 0.1025, Train SSIM: 0.8723, Val SSIM: 0.8363\n",
      "Epoch [17/100] Train Loss: 0.0826, Val Loss: 0.0927, Train SSIM: 0.8804, Val SSIM: 0.8509\n",
      "Epoch [18/100] Train Loss: 0.0795, Val Loss: 0.1231, Train SSIM: 0.8860, Val SSIM: 0.8293\n",
      "Epoch [19/100] Train Loss: 0.0805, Val Loss: 0.0765, Train SSIM: 0.8899, Val SSIM: 0.8904\n",
      "  [*] Model saved at epoch 19\n",
      "Epoch [20/100] Train Loss: 0.0752, Val Loss: 0.0798, Train SSIM: 0.8983, Val SSIM: 0.8907\n",
      "Epoch [21/100] Train Loss: 0.0683, Val Loss: 0.0660, Train SSIM: 0.9097, Val SSIM: 0.9111\n",
      "  [*] Model saved at epoch 21\n",
      "Epoch [22/100] Train Loss: 0.0768, Val Loss: 0.0793, Train SSIM: 0.8953, Val SSIM: 0.8916\n",
      "Epoch [23/100] Train Loss: 0.0726, Val Loss: 0.0965, Train SSIM: 0.9035, Val SSIM: 0.8437\n",
      "Epoch [24/100] Train Loss: 0.0677, Val Loss: 0.0640, Train SSIM: 0.9099, Val SSIM: 0.9156\n",
      "  [*] Model saved at epoch 24\n",
      "Epoch [25/100] Train Loss: 0.0620, Val Loss: 0.0661, Train SSIM: 0.9205, Val SSIM: 0.9097\n",
      "Epoch [26/100] Train Loss: 0.0621, Val Loss: 0.0594, Train SSIM: 0.9194, Val SSIM: 0.9229\n",
      "  [*] Model saved at epoch 26\n",
      "Epoch [27/100] Train Loss: 0.0674, Val Loss: 0.0641, Train SSIM: 0.9075, Val SSIM: 0.9121\n",
      "Epoch [28/100] Train Loss: 0.0612, Val Loss: 0.0663, Train SSIM: 0.9200, Val SSIM: 0.9049\n",
      "Epoch [29/100] Train Loss: 0.0594, Val Loss: 0.0785, Train SSIM: 0.9234, Val SSIM: 0.8731\n",
      "Epoch [30/100] Train Loss: 0.0626, Val Loss: 0.0733, Train SSIM: 0.9178, Val SSIM: 0.8831\n",
      "Epoch [31/100] Train Loss: 0.0598, Val Loss: 0.0567, Train SSIM: 0.9194, Val SSIM: 0.9243\n",
      "  [*] Model saved at epoch 31\n",
      "Epoch [32/100] Train Loss: 0.0561, Val Loss: 0.0563, Train SSIM: 0.9275, Val SSIM: 0.9272\n",
      "  [*] Model saved at epoch 32\n",
      "Epoch [33/100] Train Loss: 0.0579, Val Loss: 0.0597, Train SSIM: 0.9254, Val SSIM: 0.9145\n",
      "Epoch [34/100] Train Loss: 0.0562, Val Loss: 0.0595, Train SSIM: 0.9265, Val SSIM: 0.9178\n",
      "Epoch [35/100] Train Loss: 0.0569, Val Loss: 0.0672, Train SSIM: 0.9251, Val SSIM: 0.9108\n",
      "Epoch [36/100] Train Loss: 0.0590, Val Loss: 0.0563, Train SSIM: 0.9199, Val SSIM: 0.9263\n",
      "Epoch [37/100] Train Loss: 0.0542, Val Loss: 0.0550, Train SSIM: 0.9305, Val SSIM: 0.9286\n",
      "  [*] Model saved at epoch 37\n",
      "Epoch [38/100] Train Loss: 0.0532, Val Loss: 0.0540, Train SSIM: 0.9313, Val SSIM: 0.9292\n",
      "  [*] Model saved at epoch 38\n",
      "Epoch [39/100] Train Loss: 0.0529, Val Loss: 0.0589, Train SSIM: 0.9302, Val SSIM: 0.9190\n",
      "Epoch [40/100] Train Loss: 0.0516, Val Loss: 0.0521, Train SSIM: 0.9323, Val SSIM: 0.9316\n",
      "  [*] Model saved at epoch 40\n",
      "Epoch [41/100] Train Loss: 0.0506, Val Loss: 0.0519, Train SSIM: 0.9346, Val SSIM: 0.9317\n",
      "  [*] Model saved at epoch 41\n",
      "Epoch [42/100] Train Loss: 0.0505, Val Loss: 0.0608, Train SSIM: 0.9347, Val SSIM: 0.9090\n",
      "Epoch [43/100] Train Loss: 0.0515, Val Loss: 0.0894, Train SSIM: 0.9333, Val SSIM: 0.8316\n",
      "Epoch [44/100] Train Loss: 0.0517, Val Loss: 0.0577, Train SSIM: 0.9322, Val SSIM: 0.9164\n",
      "Epoch [45/100] Train Loss: 0.0513, Val Loss: 0.0869, Train SSIM: 0.9325, Val SSIM: 0.8521\n",
      "Epoch [46/100] Train Loss: 0.0506, Val Loss: 0.0576, Train SSIM: 0.9327, Val SSIM: 0.9142\n",
      "Epoch [47/100] Train Loss: 0.0555, Val Loss: 0.0509, Train SSIM: 0.9245, Val SSIM: 0.9345\n",
      "  [*] Model saved at epoch 47\n",
      "Epoch [48/100] Train Loss: 0.0522, Val Loss: 0.0513, Train SSIM: 0.9291, Val SSIM: 0.9344\n",
      "Epoch [49/100] Train Loss: 0.0494, Val Loss: 0.0507, Train SSIM: 0.9356, Val SSIM: 0.9317\n",
      "  [*] Model saved at epoch 49\n",
      "Epoch [50/100] Train Loss: 0.0489, Val Loss: 0.0569, Train SSIM: 0.9353, Val SSIM: 0.9294\n",
      "Epoch [51/100] Train Loss: 0.0525, Val Loss: 0.0655, Train SSIM: 0.9294, Val SSIM: 0.9051\n",
      "Epoch [52/100] Train Loss: 0.0510, Val Loss: 0.0551, Train SSIM: 0.9321, Val SSIM: 0.9229\n",
      "Epoch [53/100] Train Loss: 0.0503, Val Loss: 0.0509, Train SSIM: 0.9336, Val SSIM: 0.9328\n",
      "Epoch [54/100] Train Loss: 0.0490, Val Loss: 0.0506, Train SSIM: 0.9350, Val SSIM: 0.9351\n",
      "  [*] Model saved at epoch 54\n",
      "Epoch [55/100] Train Loss: 0.0472, Val Loss: 0.0483, Train SSIM: 0.9379, Val SSIM: 0.9374\n",
      "  [*] Model saved at epoch 55\n",
      "Epoch [56/100] Train Loss: 0.0482, Val Loss: 0.0487, Train SSIM: 0.9363, Val SSIM: 0.9343\n",
      "Epoch [57/100] Train Loss: 0.0486, Val Loss: 0.0587, Train SSIM: 0.9360, Val SSIM: 0.9076\n",
      "Epoch [58/100] Train Loss: 0.0477, Val Loss: 0.0575, Train SSIM: 0.9370, Val SSIM: 0.9158\n",
      "Epoch [59/100] Train Loss: 0.0469, Val Loss: 0.0507, Train SSIM: 0.9394, Val SSIM: 0.9300\n",
      "Epoch [60/100] Train Loss: 0.0473, Val Loss: 0.0510, Train SSIM: 0.9369, Val SSIM: 0.9282\n",
      "Epoch [61/100] Train Loss: 0.0468, Val Loss: 0.0475, Train SSIM: 0.9378, Val SSIM: 0.9366\n",
      "  [*] Model saved at epoch 61\n",
      "Epoch [62/100] Train Loss: 0.0454, Val Loss: 0.0490, Train SSIM: 0.9409, Val SSIM: 0.9335\n",
      "Epoch [63/100] Train Loss: 0.0486, Val Loss: 0.0561, Train SSIM: 0.9327, Val SSIM: 0.9134\n",
      "Epoch [64/100] Train Loss: 0.0478, Val Loss: 0.0497, Train SSIM: 0.9364, Val SSIM: 0.9313\n",
      "Epoch [65/100] Train Loss: 0.0471, Val Loss: 0.0561, Train SSIM: 0.9384, Val SSIM: 0.9165\n",
      "Epoch [66/100] Train Loss: 0.0474, Val Loss: 0.0504, Train SSIM: 0.9372, Val SSIM: 0.9311\n",
      "Epoch [67/100] Train Loss: 0.0517, Val Loss: 0.0550, Train SSIM: 0.9313, Val SSIM: 0.9285\n",
      "Epoch [68/100] Train Loss: 0.0472, Val Loss: 0.0490, Train SSIM: 0.9386, Val SSIM: 0.9339\n",
      "Epoch [69/100] Train Loss: 0.0457, Val Loss: 0.0472, Train SSIM: 0.9396, Val SSIM: 0.9377\n",
      "  [*] Model saved at epoch 69\n",
      "Epoch [70/100] Train Loss: 0.0482, Val Loss: 0.0466, Train SSIM: 0.9340, Val SSIM: 0.9390\n",
      "  [*] Model saved at epoch 70\n",
      "Epoch [71/100] Train Loss: 0.0445, Val Loss: 0.0475, Train SSIM: 0.9426, Val SSIM: 0.9356\n",
      "Epoch [72/100] Train Loss: 0.0447, Val Loss: 0.0489, Train SSIM: 0.9414, Val SSIM: 0.9323\n",
      "Epoch [73/100] Train Loss: 0.0440, Val Loss: 0.0453, Train SSIM: 0.9423, Val SSIM: 0.9404\n",
      "  [*] Model saved at epoch 73\n",
      "Epoch [74/100] Train Loss: 0.0440, Val Loss: 0.0448, Train SSIM: 0.9419, Val SSIM: 0.9416\n",
      "  [*] Model saved at epoch 74\n",
      "Epoch [75/100] Train Loss: 0.0437, Val Loss: 0.0455, Train SSIM: 0.9423, Val SSIM: 0.9409\n",
      "Epoch [76/100] Train Loss: 0.0463, Val Loss: 0.0564, Train SSIM: 0.9377, Val SSIM: 0.9118\n",
      "Epoch [77/100] Train Loss: 0.0447, Val Loss: 0.0606, Train SSIM: 0.9396, Val SSIM: 0.8980\n",
      "Epoch [78/100] Train Loss: 0.0433, Val Loss: 0.0488, Train SSIM: 0.9429, Val SSIM: 0.9306\n",
      "Epoch [79/100] Train Loss: 0.0432, Val Loss: 0.0439, Train SSIM: 0.9428, Val SSIM: 0.9423\n",
      "  [*] Model saved at epoch 79\n",
      "Epoch [80/100] Train Loss: 0.0434, Val Loss: 0.0449, Train SSIM: 0.9435, Val SSIM: 0.9410\n",
      "Epoch [81/100] Train Loss: 0.0421, Val Loss: 0.0442, Train SSIM: 0.9446, Val SSIM: 0.9422\n",
      "Epoch [82/100] Train Loss: 0.0422, Val Loss: 0.0437, Train SSIM: 0.9448, Val SSIM: 0.9431\n",
      "  [*] Model saved at epoch 82\n",
      "Epoch [83/100] Train Loss: 0.0425, Val Loss: 0.0451, Train SSIM: 0.9441, Val SSIM: 0.9394\n",
      "Epoch [84/100] Train Loss: 0.0454, Val Loss: 0.0594, Train SSIM: 0.9394, Val SSIM: 0.9108\n",
      "Epoch [85/100] Train Loss: 0.0458, Val Loss: 0.0496, Train SSIM: 0.9393, Val SSIM: 0.9290\n",
      "Epoch [86/100] Train Loss: 0.0443, Val Loss: 0.0489, Train SSIM: 0.9401, Val SSIM: 0.9338\n",
      "Epoch [87/100] Train Loss: 0.0443, Val Loss: 0.0499, Train SSIM: 0.9428, Val SSIM: 0.9281\n",
      "Epoch [88/100] Train Loss: 0.0440, Val Loss: 0.0501, Train SSIM: 0.9419, Val SSIM: 0.9280\n",
      "Epoch [89/100] Train Loss: 0.0438, Val Loss: 0.0431, Train SSIM: 0.9408, Val SSIM: 0.9445\n",
      "  [*] Model saved at epoch 89\n",
      "Epoch [90/100] Train Loss: 0.0422, Val Loss: 0.0507, Train SSIM: 0.9442, Val SSIM: 0.9252\n",
      "Epoch [91/100] Train Loss: 0.0417, Val Loss: 0.0435, Train SSIM: 0.9456, Val SSIM: 0.9416\n",
      "Epoch [92/100] Train Loss: 0.0418, Val Loss: 0.0441, Train SSIM: 0.9454, Val SSIM: 0.9427\n",
      "Epoch [93/100] Train Loss: 0.0408, Val Loss: 0.0428, Train SSIM: 0.9467, Val SSIM: 0.9432\n",
      "  [*] Model saved at epoch 93\n",
      "Epoch [94/100] Train Loss: 0.0421, Val Loss: 0.0423, Train SSIM: 0.9443, Val SSIM: 0.9438\n",
      "  [*] Model saved at epoch 94\n",
      "Epoch [95/100] Train Loss: 0.0419, Val Loss: 0.0485, Train SSIM: 0.9447, Val SSIM: 0.9390\n",
      "Epoch [96/100] Train Loss: 0.0434, Val Loss: 0.0425, Train SSIM: 0.9420, Val SSIM: 0.9448\n",
      "Epoch [97/100] Train Loss: 0.0408, Val Loss: 0.0423, Train SSIM: 0.9463, Val SSIM: 0.9446\n",
      "Epoch [98/100] Train Loss: 0.0422, Val Loss: 0.0427, Train SSIM: 0.9430, Val SSIM: 0.9425\n",
      "Epoch [99/100] Train Loss: 0.0422, Val Loss: 0.0528, Train SSIM: 0.9455, Val SSIM: 0.9228\n",
      "Epoch [100/100] Train Loss: 0.0404, Val Loss: 0.0411, Train SSIM: 0.9470, Val SSIM: 0.9454\n",
      "  [*] Model saved at epoch 100\n",
      "Training complete. Best model saved as 'embryo_unet_fusion.pth'.\n",
      "\n",
      "SSIM Scores for Plotting:\n",
      "Train SSIM Scores: [0.4307760855159846, 0.6259803577172396, 0.6827319829756159, 0.6978174037238845, 0.7073706056042632, 0.8064425500840955, 0.7743567771428855, 0.7976535751383428, 0.8339812403038595, 0.8524014564432852, 0.8628519958755263, 0.8684877750611009, 0.8443225290804942, 0.8721619567058014, 0.8707164679496793, 0.8723395583261182, 0.8803563235496331, 0.8859637093798094, 0.8898847851617722, 0.8982640681639446, 0.9097426514642488, 0.8953246109015565, 0.903466587274155, 0.9099049966246366, 0.9204739413718771, 0.9194000664019881, 0.9074618245738014, 0.9199886971002785, 0.9233792328919229, 0.9178002642907639, 0.9193692088762354, 0.9275112136219154, 0.9254314937557675, 0.9265260943087549, 0.9251016987153523, 0.9199088876454911, 0.9305380301196029, 0.9312709804961778, 0.9302355577213091, 0.9323254098477203, 0.9345507599532499, 0.934736329645289, 0.9332708161635255, 0.9322256072164641, 0.9324774963394151, 0.9326834072016484, 0.9245013704731875, 0.9291238877955381, 0.9356424968365454, 0.9353217662545334, 0.9294358104414457, 0.9320618829972791, 0.9335539971956351, 0.9349868709410063, 0.9379284894275834, 0.9363160712469111, 0.9359990052389209, 0.9369517499134341, 0.9394385131807141, 0.9368715410122439, 0.9378278924559191, 0.9409329985428873, 0.9327058257472239, 0.9363769197548685, 0.9383623820215093, 0.9371627084315353, 0.9312576109520284, 0.9385548163053198, 0.9395871302162564, 0.9339549407018861, 0.9426331562428449, 0.9414176225238753, 0.9422791367314002, 0.9419136695285886, 0.9422947048717452, 0.9377278274795302, 0.9395906123767738, 0.9428811303040486, 0.9427876249088064, 0.9435341793309309, 0.9446328774650398, 0.9447832933327657, 0.9440712272379792, 0.9393864322725028, 0.9392966072258687, 0.9400622587432658, 0.9428486409026395, 0.9419448799180731, 0.9408354251041717, 0.9441662956935686, 0.9455606949054115, 0.9453511967540422, 0.9467076191258473, 0.9442598959480679, 0.9447406029616537, 0.9419562932017853, 0.9463052496605191, 0.9430196428171906, 0.9454677317748061, 0.9469629215940065]\n",
      "Validation SSIM Scores: [0.6165852318418786, 0.6283292575930872, 0.7006570637648832, 0.7003917330545737, 0.8304867101899276, 0.8519514748390685, 0.8123767731037546, 0.7579313694162572, 0.8696024874423413, 0.8684669437137902, 0.8668534096251143, 0.8395424548615801, 0.8621345859892825, 0.8449711554439355, 0.8706546104546135, 0.836303691069285, 0.8509496568788028, 0.8292845360776211, 0.8904091703130844, 0.8907455983736836, 0.9110515409327568, 0.891640231964436, 0.8437092215456861, 0.9155529881200046, 0.9096817556002461, 0.9228536820580774, 0.9120614300382898, 0.9049319819355688, 0.8731271091927874, 0.8830616803879433, 0.9242513370006642, 0.9271886040132942, 0.9144827254275059, 0.9177765626433894, 0.9108058091596509, 0.9262972875689783, 0.9285784744201823, 0.929154044347452, 0.9190179574574139, 0.931562104123704, 0.931737945434895, 0.9089678990925457, 0.8316015973158762, 0.916354867583471, 0.8520637471625145, 0.9142383125656885, 0.9344682101662277, 0.9343765077861488, 0.931671456665012, 0.9293794281093787, 0.9051262436183631, 0.9229032156315256, 0.9328315976663684, 0.9350711639891279, 0.9374001364335947, 0.9343001512770958, 0.9076145236373793, 0.9157658988702382, 0.929993211800325, 0.9281772176424662, 0.936566856735987, 0.9335029852305744, 0.9134238721631097, 0.9313424567804269, 0.9164689961054646, 0.9311089921504894, 0.9284864596441282, 0.9339130220683753, 0.937677560123146, 0.9390181632752114, 0.9356183177190469, 0.9322776650706082, 0.9403920283554293, 0.9416066923885481, 0.9409321732554875, 0.9118057176576438, 0.8980013385732123, 0.9305626010218411, 0.9422859325476572, 0.940975215840847, 0.9421770792480901, 0.9430754260813936, 0.9393787367124085, 0.9107557894490289, 0.929047685565678, 0.9338002221804138, 0.9280566746461476, 0.9280056395429246, 0.9444688939033671, 0.9251735015963831, 0.9416245412319264, 0.942673331456827, 0.9431996802066235, 0.9438119416541242, 0.9390139089408496, 0.9447572840021011, 0.944617620596649, 0.9424654710377361, 0.9227778953863374, 0.9453616201454866]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d79bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model weights for testing.\n"
     ]
    }
   ],
   "source": [
    "test_image_paths = [\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F15\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-15\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F30\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-30\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F45\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-45\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(in_channels=6, out_channels=1).to(device)\n",
    "model.load_state_dict(torch.load('embryo_unet_fusion.pth', map_location=device))\n",
    "print(\"Loaded trained model weights for testing.\")\n",
    "fused_image = test_single_embryo(model, test_image_paths, transform, device)\n",
    "fused_image.save(\"fused_output.jpg\")\n",
    "fused_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660546c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embryo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
