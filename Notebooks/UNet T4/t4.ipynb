{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 704 embryos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\Embryo\\embryo_env\\Lib\\site-packages\\albumentations\\core\\validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_14500\\2398922987.py:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() if use_amp else None\n",
      "c:\\Projects\\Embryo\\embryo_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Projects\\Embryo\\embryo_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\parth/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16\n",
    "from pytorch_msssim import ms_ssim\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Reproducibility\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Utility: common embryo IDs\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def get_common_embryo_ids(base_paths):\n",
    "    sets = [set(os.listdir(p)) for p in base_paths]\n",
    "    common = set.intersection(*sets)\n",
    "    return sorted(common)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3. U-Net + Residual Head\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch), nn.ReLU(True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch), nn.ReLU(True),\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=6, out_ch=1):\n",
    "        super().__init__()\n",
    "        self.enc1 = DoubleConv(in_ch, 64)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.enc2 = DoubleConv(64,128)\n",
    "        self.enc3 = DoubleConv(128,256)\n",
    "        self.enc4 = DoubleConv(256,512)\n",
    "        self.enc5 = DoubleConv(512,1024)\n",
    "        self.up  = lambda ic, oc: nn.ConvTranspose2d(ic, oc, 2, 2)\n",
    "        self.dec4 = DoubleConv(1024+512, 512)\n",
    "        self.dec3 = DoubleConv(512+256, 256)\n",
    "        self.dec2 = DoubleConv(256+128, 128)\n",
    "        self.dec1 = DoubleConv(128+64, 64)\n",
    "        self.outc= nn.Conv2d(64, out_ch, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        c1 = self.enc1(x); p1 = self.pool(c1)\n",
    "        c2 = self.enc2(p1); p2 = self.pool(c2)\n",
    "        c3 = self.enc3(p2); p3 = self.pool(c3)\n",
    "        c4 = self.enc4(p3); p4 = self.pool(c4)\n",
    "        c5 = self.enc5(p4)\n",
    "        u4= self.up(1024,512)(c5); d4= self.dec4(torch.cat([u4,c4],1))\n",
    "        u3= self.up(512,256)(d4);  d3= self.dec3(torch.cat([u3,c3],1))\n",
    "        u2= self.up(256,128)(d3);  d2= self.dec2(torch.cat([u2,c2],1))\n",
    "        u1= self.up(128,64)(d2);   d1= self.dec1(torch.cat([u1,c1],1))\n",
    "        return self.outc(d1)\n",
    "\n",
    "class UNetResidual(UNet):\n",
    "    def forward(self,x):\n",
    "        # baseline: average focal stack\n",
    "        base = x.mean(dim=1, keepdim=True)   # (N,1,H,W)\n",
    "        res  = super().forward(x)            # (N,1,H,W) residual\n",
    "        fused= torch.clamp(base + res, 0.0, 1.0)\n",
    "        return fused\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Dataset\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "class EmbryoT4Dataset(Dataset):\n",
    "    def __init__(self, base_paths, target_path, phase_csv_dir,\n",
    "                 embryo_ids, transform=None,\n",
    "                 num_t4_embryos=50, num_other_embryos=0):\n",
    "        assert len(base_paths)==6, \"Need 6 focal dirs\"\n",
    "        self.base_paths, self.target_path = base_paths, target_path\n",
    "        self.phase_dir, self.transform = phase_csv_dir, transform\n",
    "\n",
    "        # find t4 windows\n",
    "        t4s=[]\n",
    "        for eid in embryo_ids:\n",
    "            csv = os.path.join(phase_csv_dir, f\"{eid}_phases.csv\")\n",
    "            if not os.path.exists(csv): continue\n",
    "            df = pd.read_csv(csv, header=None, names=['phase','s','e'])\n",
    "            r = df[df.phase=='t4']\n",
    "            if not r.empty and r.s.iloc[0]<=r.e.iloc[0]:\n",
    "                t4s.append((eid,r.s.iloc[0],r.e.iloc[0]))\n",
    "        self.t4s = random.sample(t4s, min(num_t4_embryos,len(t4s)))\n",
    "        t4ids = {eid for eid,_,_ in self.t4s}\n",
    "        others= [eid for eid in embryo_ids if eid not in t4ids]\n",
    "        self.others= random.sample(others, min(num_other_embryos,len(others)))\n",
    "\n",
    "        # build samples\n",
    "        self.samples=[]\n",
    "        for eid,s,e in self.t4s:\n",
    "            files = sorted(os.listdir(os.path.join(base_paths[0],eid)))\n",
    "            for i,fn in enumerate(files,1):\n",
    "                if s<=i<=e: self.samples.append((eid,fn))\n",
    "        for eid in self.others:\n",
    "            files = sorted(os.listdir(os.path.join(base_paths[0],eid)))\n",
    "            pick = random.sample(files, min(2,len(files)))\n",
    "            for fn in pick: self.samples.append((eid,fn))\n",
    "\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(\"No samples found.\")\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        eid,fn = self.samples[idx]\n",
    "        # load 6 focals\n",
    "        focals=[]\n",
    "        for p in self.base_paths:\n",
    "            img = Image.open(os.path.join(p,eid,fn)).convert('L')\n",
    "            focals.append(np.array(img))\n",
    "        x = np.stack(focals,axis=-1)  # H,W,6\n",
    "        # load target F0\n",
    "        tgt = Image.open(os.path.join(self.target_path, eid,fn)).convert('L')\n",
    "        tgt = np.array(tgt)[...,None]  # H,W,1\n",
    "\n",
    "        aug = self.transform(image=x, target=tgt)\n",
    "        inp = aug['image'].float()     # 6,H,W\n",
    "        out = aug['target'].float()    # 1,H,W\n",
    "        return inp, out\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Losses: L1 + MS-SSIM + Perceptual\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# VGG feature extractor\n",
    "def make_vgg(device):\n",
    "    vgg = vgg16(pretrained=True).features[:9].to(device).eval()  # up to conv2_2\n",
    "    for p in vgg.parameters(): p.requires_grad=False\n",
    "    return vgg\n",
    "\n",
    "def perceptual_loss(pred, tgt, vgg):\n",
    "    # rep 3-channel\n",
    "    p = vgg(pred.repeat(1,3,1,1))\n",
    "    t = vgg(tgt.repeat(1,3,1,1))\n",
    "    return F.l1_loss(p, t)\n",
    "\n",
    "def combined_fusion_loss(pred, tgt, vgg):\n",
    "    l1   = F.l1_loss(pred, tgt)\n",
    "    ssim = 1 - ms_ssim(pred, tgt, data_range=1.0, size_average=True)\n",
    "    perc = perceptual_loss(pred, tgt, vgg)\n",
    "    return l1 + 0.5*ssim + 0.1*perc\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Training loop\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def train_model(model, train_loader, val_loader,\n",
    "                epochs=50, device='cuda', lr=1e-4, use_amp=True):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                     mode='min',\n",
    "                                                     patience=3,\n",
    "                                                     factor=0.5)\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    vgg    = make_vgg(device)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    for ep in range(1, epochs+1):\n",
    "        #---- train ----\n",
    "        model.train()\n",
    "        running = 0\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    pred = model(x)\n",
    "                    loss = combined_fusion_loss(pred, y, vgg)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                pred = model(x)\n",
    "                loss = combined_fusion_loss(pred, y, vgg)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            running += loss.item() * x.size(0)\n",
    "        train_loss = running / len(train_loader.dataset)\n",
    "\n",
    "        #---- validate ----\n",
    "        model.eval()\n",
    "        running = 0\n",
    "        with torch.no_grad():\n",
    "            for x,y in val_loader:\n",
    "                x,y = x.to(device), y.to(device)\n",
    "                pred = model(x)\n",
    "                running += combined_fusion_loss(pred, y, vgg).item() * x.size(0)\n",
    "        val_loss = running / len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {ep}/{epochs}  Train: {train_loss:.4f}  Val: {val_loss:.4f}\")\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), 'embryo_unet_t4_residual.pth')\n",
    "            print(\"  [*] Saved new best model\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7. Main entry\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def main_train(seed=42):\n",
    "    set_seed(seed)\n",
    "\n",
    "    base_paths = [\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F15\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-15\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F30\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-30\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F45\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-45\"\n",
    "    ]\n",
    "    target_path   = r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F0\"\n",
    "    phase_csv_dir = r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_annotations\"\n",
    "\n",
    "    embryo_ids = get_common_embryo_ids(base_paths)\n",
    "    print(f\"Found {len(embryo_ids)} embryos\")\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.RandomRotate90(),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        A.Resize(256,256),\n",
    "        A.Normalize(mean=[0.5]*6, std=[0.5]*6),\n",
    "        ToTensorV2()\n",
    "    ], additional_targets={'target': 'image'})\n",
    "\n",
    "    dataset = EmbryoT4Dataset(\n",
    "        base_paths, target_path, phase_csv_dir,\n",
    "        embryo_ids, transform,\n",
    "        num_t4_embryos=50, num_other_embryos=0\n",
    "    )\n",
    "    train_len = int(0.8*len(dataset))\n",
    "    val_len   = len(dataset)-train_len\n",
    "    train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "    model = UNetResidual(in_ch=6, out_ch=1).to(device)\n",
    "\n",
    "    train_model(model, train_loader, val_loader,\n",
    "                epochs=50, device=device, lr=1e-4, use_amp=True)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_train(seed=121)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b79d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embryo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
