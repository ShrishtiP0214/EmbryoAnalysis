{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f265c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:45: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:45: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_32324\\3704589240.py:45: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  focal_paths = {fp: os.path.join(base_path, f'C:\\Projects\\Embryo\\Dataset\\embryo_dataset_{fp}') for fp in focal_planes}\n",
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_32324\\3704589240.py:45: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  focal_paths = {fp: os.path.join(base_path, f'C:\\Projects\\Embryo\\Dataset\\embryo_dataset_{fp}') for fp in focal_planes}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 common embryo IDs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embryo_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m common embryo IDs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Split into training and validation sets\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m train_ids, val_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43membryo_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Custom Dataset class\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mEmbryoDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[1;32mc:\\Projects\\Embryo\\embryo_env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Projects\\Embryo\\embryo_env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2851\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2848\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2850\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2851\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Projects\\Embryo\\embryo_env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2481\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2478\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2485\u001b[0m     )\n\u001b[0;32m   2487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import threshold_otsu\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from segmentation_models_pytorch import Unet\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Utility function to find common embryo IDs\n",
    "def get_common_embryo_ids(base_paths):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of folder names (embryo IDs)\n",
    "    that appear in *all* the given directories.\n",
    "    \"\"\"\n",
    "    sets_of_ids = []\n",
    "    for path in base_paths:\n",
    "        subfolders = [\n",
    "            d for d in os.listdir(path)\n",
    "            if os.path.isdir(os.path.join(path, d))\n",
    "        ]\n",
    "        sets_of_ids.append(set(subfolders))\n",
    "    common_ids = set.intersection(*sets_of_ids)\n",
    "    return sorted(list(common_ids))\n",
    "\n",
    "# Manual Dice coefficient implementation\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    pred = pred.contiguous().view(-1)\n",
    "    target = target.contiguous().view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "\n",
    "# Dataset paths\n",
    "base_path = 'Dataset'\n",
    "annotations_path = os.path.join(base_path, r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_annotations\")\n",
    "gt_path = os.path.join(base_path, r\"C:\\Projects\\Embryo\\Dataset\")\n",
    "focal_planes = ['F15', 'F30', 'F45', 'F-15', 'F-30', 'F-45']\n",
    "focal_paths = {fp: os.path.join(base_path, f'C:\\Projects\\Embryo\\Dataset\\embryo_dataset_{fp}') for fp in focal_planes}\n",
    "\n",
    "# Image size\n",
    "H, W = 256, 256\n",
    "N_channels = len(focal_planes)  # 6 focal planes\n",
    "\n",
    "# Get common embryo IDs across all directories\n",
    "all_paths = [annotations_path, gt_path] + list(focal_paths.values())\n",
    "embryo_ids = get_common_embryo_ids(all_paths)\n",
    "print(f'Found {len(embryo_ids)} common embryo IDs')\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_ids, val_ids = train_test_split(embryo_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Dataset class\n",
    "class EmbryoDataset(Dataset):\n",
    "    def __init__(self, embryo_ids, annotations_path, focal_paths, gt_path, transform=None):\n",
    "        self.embryo_ids = embryo_ids\n",
    "        self.annotations_path = annotations_path\n",
    "        self.focal_paths = focal_paths\n",
    "        self.gt_path = gt_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embryo_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embryo_id = self.embryo_ids[idx]\n",
    "        # Read annotation to get t4 frame\n",
    "        csv_file = os.path.join(self.annotations_path, f'{embryo_id}.csv')\n",
    "        df = pd.read_csv(csv_file)\n",
    "        t4_frame = int(df['t4'].iloc[0])  # Use t4 frame\n",
    "        # Load 6 focal plane images for t4 frame\n",
    "        input_stack = []\n",
    "        for fp in focal_planes:\n",
    "            img_file = os.path.join(self.focal_paths[fp], f'{embryo_id}_frame_{t4_frame:03d}.png')\n",
    "            if not os.path.exists(img_file):\n",
    "                raise FileNotFoundError(f'Image {img_file} not found')\n",
    "            img = imread(img_file, as_gray=True)\n",
    "            img = resize(img, (H, W), preserve_range=True) / 255.0\n",
    "            input_stack.append(img)\n",
    "        input_stack = np.stack(input_stack, axis=-1)  # Shape: (H, W, 6)\n",
    "        # Load ground truth image and convert to binary mask\n",
    "        gt_file = os.path.join(self.gt_path, f'{embryo_id}_gt.png')\n",
    "        gt_img = imread(gt_file, as_gray=True)\n",
    "        gt_img = resize(gt_img, (H, W), preserve_range=True)\n",
    "        thresh = threshold_otsu(gt_img)\n",
    "        gt_mask = (gt_img > thresh).astype(np.float32)  # Shape: (H, W)\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=input_stack, mask=gt_mask)\n",
    "            input_stack = augmented['image']\n",
    "            gt_mask = augmented['mask']\n",
    "        else:\n",
    "            input_stack = torch.tensor(input_stack).permute(2, 0, 1)  # (6, H, W)\n",
    "            gt_mask = torch.tensor(gt_mask).unsqueeze(0)  # (1, H, W)\n",
    "        return input_stack, gt_mask\n",
    "\n",
    "# Define transformations\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = EmbryoDataset(train_ids, annotations_path, focal_paths, gt_path, transform=train_transform)\n",
    "val_dataset = EmbryoDataset(val_ids, annotations_path, focal_paths, gt_path, transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define U-Net model\n",
    "model = Unet(encoder_name='resnet34', in_channels=N_channels, classes=1)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_dice = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            preds = (outputs > 0.5).float()\n",
    "            val_dice += dice_coefficient(preds, targets).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_dice /= len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}')\n",
    "\n",
    "    # Early stopping and model checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "print('Training completed. Best model saved as best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1078bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embryo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
