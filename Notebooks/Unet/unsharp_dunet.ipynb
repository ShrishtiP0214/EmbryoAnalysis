{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386b86b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 704 embryo IDs: ['AA83-7', 'AAL839-6', 'AB028-6', 'AB91-1', 'AC264-1'] ...\n",
      "Using device: cuda\n",
      "Starting training...\n",
      "Epoch [1/10] Train Loss: 0.0875, Val Loss: 0.0384\n",
      "  [*] Model saved at epoch 1\n",
      "Epoch [2/10] Train Loss: 0.0372, Val Loss: 0.0324\n",
      "  [*] Model saved at epoch 2\n",
      "Epoch [3/10] Train Loss: 0.0342, Val Loss: 0.0189\n",
      "  [*] Model saved at epoch 3\n",
      "Epoch [4/10] Train Loss: 0.0242, Val Loss: 0.0150\n",
      "  [*] Model saved at epoch 4\n",
      "Epoch [5/10] Train Loss: 0.0254, Val Loss: 0.0192\n",
      "Epoch [6/10] Train Loss: 0.0225, Val Loss: 0.0182\n",
      "Epoch [7/10] Train Loss: 0.0240, Val Loss: 0.0111\n",
      "  [*] Model saved at epoch 7\n",
      "Epoch [8/10] Train Loss: 0.0230, Val Loss: 0.0224\n",
      "Epoch [9/10] Train Loss: 0.0221, Val Loss: 0.0138\n",
      "Epoch [10/10] Train Loss: 0.0170, Val Loss: 0.0088\n",
      "  [*] Model saved at epoch 10\n",
      "Training complete. Best model saved as 'embryo_unet_fusion.pth'.\n",
      "Loaded trained model weights for testing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile, ImageFilter\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Utility function to find common embryo IDs across directories\n",
    "def get_common_embryo_ids(base_paths):\n",
    "    sets_of_ids = []\n",
    "    for path in base_paths:\n",
    "        subfolders = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "        sets_of_ids.append(set(subfolders))\n",
    "    common_ids = set.intersection(*sets_of_ids)\n",
    "    return sorted(list(common_ids))\n",
    "\n",
    "# Attention Block for U-Net\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# DoubleConv Block for U-Net\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# U-Net with Attention Gates\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=6, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.conv1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.conv5 = DoubleConv(512, 1024)\n",
    "        self.up6 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.att6 = AttentionBlock(F_g=512, F_l=512, F_int=256)\n",
    "        self.conv6 = DoubleConv(1024, 512)\n",
    "        self.up7 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.att7 = AttentionBlock(F_g=256, F_l=256, F_int=128)\n",
    "        self.conv7 = DoubleConv(512, 256)\n",
    "        self.up8 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.att8 = AttentionBlock(F_g=128, F_l=128, F_int=64)\n",
    "        self.conv8 = DoubleConv(256, 128)\n",
    "        self.up9 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.att9 = AttentionBlock(F_g=64, F_l=64, F_int=32)\n",
    "        self.conv9 = DoubleConv(128, 64)\n",
    "        self.conv10 = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1(x)  # 256x256\n",
    "        p1 = self.pool1(c1)  # 128x128\n",
    "        c2 = self.conv2(p1)  # 128x128\n",
    "        p2 = self.pool2(c2)  # 64x64\n",
    "        c3 = self.conv3(p2)  # 64x64\n",
    "        p3 = self.pool3(c3)  # 32x32\n",
    "        c4 = self.conv4(p3)  # 32x32\n",
    "        p4 = self.pool4(c4)  # 16x16\n",
    "        c5 = self.conv5(p4)  # 16x16\n",
    "\n",
    "        up_6 = self.up6(c5)  # 16x16 -> 32x32\n",
    "        att_c4 = self.att6(up_6, c4)\n",
    "        merge6 = torch.cat([up_6, att_c4], dim=1)\n",
    "        c6 = self.conv6(merge6)\n",
    "\n",
    "        up_7 = self.up7(c6)  # 32x32 -> 64x64\n",
    "        att_c3 = self.att7(up_7, c3)\n",
    "        merge7 = torch.cat([up_7, att_c3], dim=1)\n",
    "        c7 = self.conv7(merge7)\n",
    "\n",
    "        up_8 = self.up8(c7)  # 64x64 -> 128x128\n",
    "        att_c2 = self.att8(up_8, c2)\n",
    "        merge8 = torch.cat([up_8, att_c2], dim=1)\n",
    "        c8 = self.conv8(merge8)\n",
    "\n",
    "        up_9 = self.up9(c8)  # 128x128 -> 256x256\n",
    "        att_c1 = self.att9(up_9, c1)\n",
    "        merge9 = torch.cat([up_9, att_c1], dim=1)\n",
    "        c9 = self.conv9(merge9)\n",
    "\n",
    "        output = self.conv10(c9)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "# Dataset for Embryo Image Fusion\n",
    "class EmbryoFocusStackDataset(Dataset):\n",
    "    def __init__(self, base_paths, embryo_ids, is_train=False):\n",
    "        self.base_paths = base_paths\n",
    "        self.embryo_ids = embryo_ids\n",
    "        self.is_train = is_train\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embryo_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embryo_id = self.embryo_ids[idx]\n",
    "        focal_pil_images = []\n",
    "\n",
    "        for path in self.base_paths:\n",
    "            embryo_subfolder = os.path.join(path, embryo_id)\n",
    "            image_files = sorted([f for f in os.listdir(embryo_subfolder) \n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            if not image_files:\n",
    "                raise FileNotFoundError(f\"No image found in {embryo_subfolder}\")\n",
    "            img_path = os.path.join(embryo_subfolder, image_files[0])\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('L')\n",
    "                focal_pil_images.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                focal_pil_images.append(Image.new('L', (256, 256), 0))\n",
    "\n",
    "        if self.is_train:\n",
    "            angle = random.uniform(-30, 30)\n",
    "            flip_h = random.random() < 0.5\n",
    "            flip_v = random.random() < 0.5\n",
    "        else:\n",
    "            angle = 0\n",
    "            flip_h = False\n",
    "            flip_v = False\n",
    "\n",
    "        transformed_images = []\n",
    "        for img in focal_pil_images:\n",
    "            img = TF.rotate(img, angle)\n",
    "            if flip_h:\n",
    "                img = TF.hflip(img)\n",
    "            if flip_v:\n",
    "                img = TF.vflip(img)\n",
    "            img = self.preprocess(img)\n",
    "            transformed_images.append(img)\n",
    "\n",
    "        focal_tensors = transformed_images\n",
    "        input_tensor = torch.cat(focal_tensors, dim=0)  # [6, H, W]\n",
    "\n",
    "        mean_target = torch.mean(torch.stack(focal_tensors), dim=0)  # [1, H, W]\n",
    "        blurred = F.avg_pool2d(mean_target.unsqueeze(0), kernel_size=3, stride=1, padding=1).squeeze(0)\n",
    "        sharp_target = mean_target + (mean_target - blurred) * 1.5  # Unsharp masking\n",
    "\n",
    "        return input_tensor, sharp_target\n",
    "\n",
    "# Loss Function\n",
    "l1_loss_fn = nn.L1Loss()\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=6, device='cuda'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 3  # Stricter early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = l1_loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = l1_loss_fn(outputs, targets)\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'embryo_unet_fusion.pth')\n",
    "            print(f\"  [*] Model saved at epoch {epoch+1}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= max_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "# Test Function with Enhanced Post-Processing\n",
    "def test_single_embryo(model, image_paths, transform, device='cuda'):\n",
    "    model.eval()\n",
    "    focal_tensors = []\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).convert('L')\n",
    "        img_tensor = transform(img)\n",
    "        focal_tensors.append(img_tensor)\n",
    "    \n",
    "    input_tensor = torch.cat(focal_tensors, dim=0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    output_image = output.squeeze(0).cpu()\n",
    "    fused_pil = transforms.ToPILImage()(output_image)\n",
    "    \n",
    "    # Refined unsharp masking\n",
    "    fused_pil = fused_pil.filter(ImageFilter.UnsharpMask(radius=3, percent=300, threshold=1))\n",
    "    fused_pil = TF.adjust_contrast(fused_pil, contrast_factor=1.5)\n",
    "    return fused_pil\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    base_paths = [\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F15\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-15\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F30\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-30\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F45\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-45\"\n",
    "    ]\n",
    "    \n",
    "    embryo_ids = get_common_embryo_ids(base_paths)\n",
    "    print(f\"Found {len(embryo_ids)} embryo IDs: {embryo_ids[:5]} ...\")\n",
    "    \n",
    "    random.seed(42)\n",
    "    train_size = int(0.8 * len(embryo_ids))\n",
    "    train_indices = random.sample(range(len(embryo_ids)), train_size)\n",
    "    val_indices = [i for i in range(len(embryo_ids)) if i not in train_indices]\n",
    "    embryo_ids_train = [embryo_ids[i] for i in train_indices]\n",
    "    embryo_ids_val = [embryo_ids[i] for i in val_indices]\n",
    "    \n",
    "    train_dataset = EmbryoFocusStackDataset(base_paths, embryo_ids_train, is_train=True)\n",
    "    val_dataset = EmbryoFocusStackDataset(base_paths, embryo_ids_val, is_train=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, num_workers=0, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, num_workers=0, shuffle=False)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = UNet(in_channels=6, out_channels=1).to(device)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, num_epochs=10, device=device)\n",
    "    print(\"Training complete. Best model saved as 'embryo_unet_fusion.pth'.\")\n",
    "\n",
    "    test_image_paths = [\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F15\\AB91-1\\D2013.01.29_S0719_I132_WELL1_RUN169.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-15\\AB91-1\\D2013.01.29_S0719_I132_WELL1_RUN169.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F45\\AB91-1\\D2013.01.29_S0719_I132_WELL1_RUN169.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-45\\AB91-1\\D2013.01.29_S0719_I132_WELL1_RUN169.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F30\\AB91-1\\D2013.01.29_S0719_I132_WELL1_RUN169.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-30\\AB91-1\\D2013.01.29_S0719_I132_WELL1_RUN169.jpeg\"\n",
    "    ]\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    model.load_state_dict(torch.load('embryo_unet_fusion.pth', map_location=device))\n",
    "    print(\"Loaded trained model weights for testing.\")\n",
    "    fused_image = test_single_embryo(model, test_image_paths, transform, device)\n",
    "    fused_image.save(\"fused_output.jpg\")\n",
    "    fused_image.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embryo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
