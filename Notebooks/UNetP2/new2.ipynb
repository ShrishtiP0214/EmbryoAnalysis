{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ec02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 704 embryo IDs: ['AA83-7', 'AAL839-6', 'AB028-6', 'AB91-1', 'AC264-1'] ...\n",
      "Total samples (embryo-frame pairs): 5630\n",
      "Total samples (embryo-frame pairs): 1410\n",
      "Using device: cuda\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_25104\\2945630797.py:237: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageFile, ImageFilter\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "import lpips\n",
    "from pytorch_msssim import SSIM\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Set up logging to file (optional)\n",
    "logging.basicConfig(filename='training_image_log.txt', level=logging.INFO, format='%(message)s')\n",
    "\n",
    "# Utility function to find common embryo IDs across directories\n",
    "def get_common_embryo_ids(base_paths):\n",
    "    sets_of_ids = []\n",
    "    for path in base_paths:\n",
    "        subfolders = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "        sets_of_ids.append(set(subfolders))\n",
    "    common_ids = set.intersection(*sets_of_ids)\n",
    "    return sorted(list(common_ids))\n",
    "\n",
    "# Utility function to get frames for an embryo\n",
    "def get_common_frames_for_embryo(embryo_id, base_paths, f0_path):\n",
    "    f0_subfolder = os.path.join(f0_path, embryo_id)\n",
    "    f0_files = sorted([f for f in os.listdir(f0_subfolder) \n",
    "                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    if not f0_files:\n",
    "        return []\n",
    "\n",
    "    common_frames = []\n",
    "    for frame in f0_files:\n",
    "        frame_exists_everywhere = True\n",
    "        for path in base_paths:\n",
    "            subfolder = os.path.join(path, embryo_id)\n",
    "            if not os.path.exists(os.path.join(subfolder, frame)):\n",
    "                frame_exists_everywhere = False\n",
    "                break\n",
    "        if frame_exists_everywhere:\n",
    "            common_frames.append(frame)\n",
    "    return common_frames\n",
    "\n",
    "# Attention Block for U-Net\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# U-Net with Attention Gates\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        def CBR(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.enc1 = CBR(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2 = CBR(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc3 = CBR(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = CBR(256, 512)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.att3 = AttentionBlock(F_g=256, F_l=256, F_int=128)\n",
    "        self.dec3 = CBR(512, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.att2 = AttentionBlock(F_g=128, F_l=128, F_int=64)\n",
    "        self.dec2 = CBR(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.att1 = AttentionBlock(F_g=64, F_l=64, F_int=32)\n",
    "        self.dec1 = CBR(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "\n",
    "        b = self.bottleneck(self.pool3(e3))\n",
    "\n",
    "        d3 = self.up3(b)\n",
    "        e3 = self.att3(d3, e3)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        e2 = self.att2(d2, e2)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        e1 = self.att1(d1, e1)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        return self.final(d1)\n",
    "\n",
    "# Dataset for Embryo Image Fusion (Handles all frames with sampling)\n",
    "class EmbryoFocusStackDataset(Dataset):\n",
    "    def __init__(self, base_paths, f0_path, embryo_ids, is_train=False, max_frames_per_embryo=10):\n",
    "        self.base_paths = base_paths\n",
    "        self.f0_path = f0_path\n",
    "        self.is_train = is_train\n",
    "        self.max_frames_per_embryo = max_frames_per_embryo\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),  # Reduced image size\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        self.data_pairs = []\n",
    "        for embryo_id in embryo_ids:\n",
    "            frames = get_common_frames_for_embryo(embryo_id, base_paths, f0_path)\n",
    "            if len(frames) > self.max_frames_per_embryo:\n",
    "                frames = random.sample(frames, self.max_frames_per_embryo)\n",
    "            for frame in frames:\n",
    "                self.data_pairs.append((embryo_id, frame))\n",
    "        \n",
    "        print(f\"Total samples (embryo-frame pairs): {len(self.data_pairs)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embryo_id, frame = self.data_pairs[idx]\n",
    "        focal_pil_images = []\n",
    "\n",
    "        for path in self.base_paths:\n",
    "            embryo_subfolder = os.path.join(path, embryo_id)\n",
    "            img_path = os.path.join(embryo_subfolder, frame)\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('L')\n",
    "                focal_pil_images.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                focal_pil_images.append(Image.new('L', (128, 128), 0))\n",
    "\n",
    "        f0_subfolder = os.path.join(self.f0_path, embryo_id)\n",
    "        f0_img_path = os.path.join(f0_subfolder, frame)\n",
    "        try:\n",
    "            f0_image = Image.open(f0_img_path).convert('L')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading F0 image {f0_img_path}: {e}\")\n",
    "            f0_image = Image.new('L', (128, 128), 0)\n",
    "\n",
    "        if self.is_train:\n",
    "            angle = random.uniform(-30, 30)\n",
    "            flip_h = random.random() < 0.5\n",
    "            flip_v = random.random() < 0.5\n",
    "        else:\n",
    "            angle = 0\n",
    "            flip_h = False\n",
    "            flip_v = False\n",
    "\n",
    "        transformed_images = []\n",
    "        for img in focal_pil_images:\n",
    "            img = TF.rotate(img, angle)\n",
    "            if flip_h:\n",
    "                img = TF.hflip(img)\n",
    "            if flip_v:\n",
    "                img = TF.vflip(img)\n",
    "            img = self.preprocess(img)\n",
    "            transformed_images.append(img)\n",
    "\n",
    "        f0_image = TF.rotate(f0_image, angle)\n",
    "        if flip_h:\n",
    "            f0_image = TF.hflip(f0_image)\n",
    "        if flip_v:\n",
    "            f0_image = TF.vflip(f0_image)\n",
    "        target_tensor = self.preprocess(f0_image)\n",
    "\n",
    "        focal_tensors = transformed_images\n",
    "        input_tensor = torch.cat(focal_tensors, dim=0)\n",
    "\n",
    "        return input_tensor, target_tensor, (embryo_id, frame)\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate(batch):\n",
    "    inputs = torch.stack([item[0] for item in batch])\n",
    "    targets = torch.stack([item[1] for item in batch])\n",
    "    embryo_frame_pairs = [item[2] for item in batch]\n",
    "    return inputs, targets, embryo_frame_pairs\n",
    "\n",
    "# Loss Functions\n",
    "l1_loss_fn = nn.L1Loss()\n",
    "ssim_loss_fn = SSIM(data_range=1.0, size_average=True, channel=1)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "\n",
    "# Training Function with Optimizations\n",
    "def train_model(model, train_loader, val_loader, base_paths, f0_path, num_epochs=1, device='cuda'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 20\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    ssim_loss_fn.to(device)\n",
    "    ssim_metric.to(device)\n",
    "\n",
    "    train_ssim_scores = []\n",
    "    val_ssim_scores = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        running_train_ssim = 0.0\n",
    "        total_batches = len(train_loader)\n",
    "        for batch_idx, (inputs, targets, embryo_frame_pairs) in enumerate(train_loader):\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}/{total_batches}\")\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                l1_loss = l1_loss_fn(outputs, targets)\n",
    "                ssim_loss = 1 - ssim_loss_fn(outputs, targets)\n",
    "                loss = 0.5 * l1_loss + 0.5 * ssim_loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            ssim_value = ssim_metric(outputs, targets)\n",
    "            running_train_ssim += ssim_value.item() * inputs.size(0)\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_ssim = running_train_ssim / len(train_loader.dataset)\n",
    "        train_ssim_scores.append(train_ssim)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_ssim = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, embryo_frame_pairs in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    l1_loss = l1_loss_fn(outputs, targets)\n",
    "                    ssim_loss = 1 - ssim_loss_fn(outputs, targets)\n",
    "                    loss = 0.5 * l1_loss + 0.5 * ssim_loss\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                ssim_value = ssim_metric(outputs, targets)\n",
    "                running_val_ssim += ssim_value.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_ssim = running_val_ssim / len(val_loader.dataset)\n",
    "        val_ssim_scores.append(val_ssim)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train SSIM: {train_ssim:.4f}, Val SSIM: {val_ssim:.4f}\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'embryo_unet_fusion.pth')\n",
    "            print(f\"  [*] Model saved at epoch {epoch+1}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= max_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    return train_ssim_scores, val_ssim_scores\n",
    "\n",
    "# Test Function with Visualization\n",
    "def test_single_embryo(model, image_paths, f0_path, transform, device='cuda'):\n",
    "    model.eval()\n",
    "    focal_tensors = []\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).convert('L')\n",
    "        img_tensor = transform(img)\n",
    "        focal_tensors.append(img_tensor)\n",
    "    \n",
    "    input_tensor = torch.cat(focal_tensors, dim=0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    output_image = output.squeeze(0).cpu()\n",
    "    fused_pil = transforms.ToPILImage()(output_image)\n",
    "    \n",
    "    fused_pil = fused_pil.filter(ImageFilter.UnsharpMask(radius=2, percent=200, threshold=3))\n",
    "    fused_pil = TF.adjust_contrast(fused_pil, contrast_factor=1.5)\n",
    "\n",
    "    input_pil = Image.open(image_paths[0]).convert('L')\n",
    "    input_pil = transform(input_pil).squeeze(0).cpu()\n",
    "    input_pil = transforms.ToPILImage()(input_pil)\n",
    "\n",
    "    embryo_id = os.path.basename(os.path.dirname(image_paths[0]))\n",
    "    f0_subfolder = os.path.join(f0_path, embryo_id)\n",
    "    frame = os.path.basename(image_paths[0])\n",
    "    f0_img_path = os.path.join(f0_subfolder, frame)\n",
    "    target_pil = Image.open(f0_img_path).convert('L')\n",
    "    target_pil = transform(target_pil).squeeze(0).cpu()\n",
    "    target_pil = transforms.ToPILImage()(target_pil)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(input_pil, cmap='gray')\n",
    "    axes[0].set_title(\"Input (F15)\")\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(target_pil, cmap='gray')\n",
    "    axes[1].set_title(\"Target (F0)\")\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(fused_pil, cmap='gray')\n",
    "    axes[2].set_title(\"Model Output\")\n",
    "    axes[2].axis('off')\n",
    "    plt.savefig(\"comparison.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return fused_pil\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    base_paths = [\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F15\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-15\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F30\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-30\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F45\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-45\"\n",
    "    ]\n",
    "    f0_path = r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset\"\n",
    "    \n",
    "    embryo_ids = get_common_embryo_ids(base_paths + [f0_path])\n",
    "    print(f\"Found {len(embryo_ids)} embryo IDs: {embryo_ids[:5]} ...\")\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    train_size = int(train_ratio * len(embryo_ids))\n",
    "    train_indices = random.sample(range(len(embryo_ids)), train_size)\n",
    "    val_indices = [i for i in range(len(embryo_ids)) if i not in train_indices]\n",
    "    embryo_ids_train = [embryo_ids[i] for i in train_indices]\n",
    "    embryo_ids_val = [embryo_ids[i] for i in val_indices]\n",
    "    \n",
    "    train_dataset = EmbryoFocusStackDataset(base_paths, f0_path, embryo_ids_train, is_train=True, max_frames_per_embryo=10)\n",
    "    val_dataset = EmbryoFocusStackDataset(base_paths, f0_path, embryo_ids_val, is_train=False, max_frames_per_embryo=10)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, num_workers=4, shuffle=True, collate_fn=custom_collate)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, num_workers=4, shuffle=False, collate_fn=custom_collate)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = UNet(in_channels=6, out_channels=1).to(device)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    train_ssim_scores, val_ssim_scores = train_model(model, train_loader, val_loader, base_paths, f0_path, num_epochs=1, device=device)\n",
    "    print(\"Training complete. Best model saved as 'embryo_unet_fusion.pth'.\")\n",
    "\n",
    "    print(\"\\nSSIM Scores for Plotting:\")\n",
    "    print(\"Train SSIM Scores:\", train_ssim_scores)\n",
    "    print(\"Validation SSIM Scores:\", val_ssim_scores)\n",
    "\n",
    "    test_image_paths = [\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F15\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-15\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F45\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-45\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F30\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\",\n",
    "        r\"C:\\Projects\\Embryo\\Dataset\\embryo_dataset_F-30\\AM716-7\\D2013.07.01_S0867_I132_WELL7_RUN209.jpeg\"\n",
    "    ]\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    model.load_state_dict(torch.load('embryo_unet_fusion.pth', map_location=device))\n",
    "    print(\"Loaded trained model weights for testing.\")\n",
    "    fused_image = test_single_embryo(model, test_image_paths, f0_path, transform, device)\n",
    "    fused_image.save(\"fused_output.jpg\")\n",
    "    fused_image.show()\n",
    "    print(\"Comparison plot saved as 'comparison.png'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69cbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embryo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
